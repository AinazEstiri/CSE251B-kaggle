{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch_geometric.data import Data, Batch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission folder if it doesn't exist\n",
    "submission_dir = './submission'\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# Uncomment the following block ONLY if you wish to inspect file paths in a Kaggle-like directory structure.\n",
    "# On your local system, you likely have the files in your local folder so this is not needed.\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "# Data Loading for Local Environment\n",
    "# Files are assumed to be in:\n",
    "# ./cse-251-b-2025/train.npz\n",
    "# ./cse-251-b-2025/test_input.npz\n",
    "\n",
    "train_file = np.load(\"./cse-251-b-2025/train.npz\")\n",
    "train_data = train_file['data']\n",
    "print(\"train_data's shape:\", train_data.shape)  # Expected shape: (10000, 50, 110, 6)\n",
    "\n",
    "test_file = np.load(\"./cse-251-b-2025/test_input.npz\")\n",
    "test_data = test_file['data']\n",
    "print(\"test_data's shape:\", test_data.shape)    # Expected shape: (2100, 50, 50, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectories from one training scene (static plot)\n",
    "data_matrix = train_data[0]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for agent in range(data_matrix.shape[0]):\n",
    "    xs = data_matrix[agent, :, 0]\n",
    "    ys = data_matrix[agent, :, 1]\n",
    "    # Remove zeros (padding)\n",
    "    xs = xs[xs != 0]\n",
    "    ys = ys[ys != 0]\n",
    "    plt.plot(xs, ys, alpha=0.7)\n",
    "plt.title(\"Trajectories from one training scene\")\n",
    "plt.xlabel(\"x-coordinate\")\n",
    "plt.ylabel(\"y-coordinate\")\n",
    "plt.show()\n",
    "\n",
    "# Create an animated gif for one training scene (exact code provided on kaggle)\n",
    "def make_gif(data_matrix, name='example'):\n",
    "    cmap = None\n",
    "    if sys.version_info.minor <= 7:\n",
    "        cmap = plt.cm.get_cmap(\"viridis\", 50)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"viridis\", 50)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    # Function to update plot for each frame\n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "        # Get data for current timestep\n",
    "        for i in range(1, data_matrix.shape[0]):\n",
    "            x = data_matrix[i, frame, 0]\n",
    "            y = data_matrix[i, frame, 1]\n",
    "            if x != 0 and y != 0:\n",
    "                xs = data_matrix[i, :frame+1, 0]  # Include current frame\n",
    "                ys = data_matrix[i, :frame+1, 1]  # Include current frame\n",
    "                # trim all zeros\n",
    "                mask = (xs != 0) & (ys != 0)  # Only keep points where both x and y are non-zero\n",
    "                xs = xs[mask]\n",
    "                ys = ys[mask]\n",
    "                # Only plot if we have points to plot\n",
    "                if len(xs) > 0 and len(ys) > 0:\n",
    "                    color = cmap(i)\n",
    "                    ax.plot(xs, ys, alpha=0.9, color=color)\n",
    "                    ax.scatter(x, y, s=80, color=color)\n",
    "        ax.plot(data_matrix[0, :frame, 0], data_matrix[0, :frame, 1],\n",
    "                color='tab:orange', label='Ego Vehicle')\n",
    "        ax.scatter(data_matrix[0, frame, 0], data_matrix[0, frame, 1],\n",
    "                   s=80, color='tab:orange')\n",
    "        # Set title with timestep\n",
    "        ax.set_title(f'Timestep {frame}')\n",
    "        # Set consistent axis limits\n",
    "        ax.set_xlim(data_matrix[:,:,0][data_matrix[:,:,0] != 0].min() - 10, \n",
    "                    data_matrix[:,:,0][data_matrix[:,:,0] != 0].max() + 10)\n",
    "        ax.set_ylim(data_matrix[:,:,1][data_matrix[:,:,1] != 0].min() - 10, \n",
    "                    data_matrix[:,:,1][data_matrix[:,:,1] != 0].max() + 10)\n",
    "        ax.legend()\n",
    "        return ax.collections + ax.lines\n",
    "\n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(fig, update, frames=list(range(0, data_matrix.shape[1], 3)),\n",
    "                                   interval=100, blit=True)\n",
    "    # Save as GIF\n",
    "    anim.save(f'trajectory_visualization_{name}.gif', writer='pillow')\n",
    "    plt.close()\n",
    "\n",
    "data_matrix = train_data[0]\n",
    "\n",
    "# make_gif(data_matrix, 'index0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant velocity from test set\n",
    "Untouched from original data loading notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this?\n",
    "run_constant_velocity_model = False\n",
    "\n",
    "if run_constant_velocity_model:\n",
    "    # Compute the velocity differences for the ego vehicle (agent index 0)\n",
    "    velocity_diff = test_data[..., 1:, :2] - test_data[..., :-1, :2]\n",
    "    print(\"Velocity difference shape:\", velocity_diff.shape)\n",
    "\n",
    "    # Compute average velocity for the ego vehicle (index 0) in each scene\n",
    "    constant_vel = np.mean(velocity_diff[:, 0, :, :], axis=1)\n",
    "    print(\"Constant velocity shape:\", constant_vel.shape)\n",
    "\n",
    "    # Generate predictions for 60 future time steps based on constant velocity\n",
    "    pred_y_const = np.zeros((test_data.shape[0], 60, 2))\n",
    "    starting_point = test_data[:, 0, -1, :2]  # Last observed position of ego vehicle\n",
    "\n",
    "    for t in range(60):\n",
    "        pred_y_const[:, t, :] = starting_point + (t + 1) * constant_vel\n",
    "\n",
    "    # Reshape predictions to submission format: (2100, 60, 2) -> (12600, 2)\n",
    "    pred_output_const = pred_y_const.reshape(-1, 2)\n",
    "    output_df_const = pd.DataFrame(pred_output_const, columns=['x', 'y'])\n",
    "    output_df_const.index.name = 'index'\n",
    "    # Save output in the submission folder\n",
    "    constant_vel_path = os.path.join(submission_dir, 'constant_vel_submission.csv')\n",
    "    output_df_const.to_csv(constant_vel_path)\n",
    "    print(f\"Constant velocity submission saved locally as '{constant_vel_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base LSTM given to us in the milestone notebook\n",
    "class BaseLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=128, output_dim=60 * 2):\n",
    "        super(BaseLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In case you passed in a DataBatch\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = x.x\n",
    "\n",
    "        x= x.reshape(-1, 50, 50, 6)  # (batch_size, num_agents, seq_len, input_dim)\n",
    "        x = x[:, 0, :, :] # Only Consider ego agent index 0\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out.view(-1, 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi agent scene context model\n",
    "class SceneContextModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.agent_encoder = nn.Sequential(\n",
    "            nn.Linear(50 * 6, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ego_encoder = nn.Sequential(\n",
    "            nn.Linear(50 * 6, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 60 * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_flat):# In case you passed in a DataBatch\n",
    "        if not isinstance(x_flat, torch.Tensor):\n",
    "            x_flat = x_flat.x\n",
    "\n",
    "        B = x_flat.size(0)\n",
    "        x = x_flat.view(B, 50, 50, 6) #(B, agents, timesteps, features)\n",
    "        x_agents = x.view(B, 50, -1)  #(B, 50, 300)\n",
    "        agent_feats = self.agent_encoder(x_agents) #(B, 50, hidden_dim)\n",
    "        scene_context = agent_feats.mean(dim=1) #(B, hidden_dim)\n",
    "\n",
    "        ego_input = x[:, 0, :, :].reshape(B, -1) #(B, 300)\n",
    "        ego_feat = self.ego_encoder(ego_input) #(B, hidden_dim)\n",
    "\n",
    "        combined = torch.cat([ego_feat, scene_context], dim=1)\n",
    "        return self.decoder(combined) #(B, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDatasetTrain(Dataset):\n",
    "    def __init__(self, data, scale=10.0, augment=True):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Training data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        augment: Whether to apply data augmentation (only for training)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]\n",
    "        # Getting 50 historical timestamps and 60 future timestamps\n",
    "        hist = scene[:, :50, :].copy()    # (agents=50, time_seq=50, 6)\n",
    "        future = torch.tensor(scene[0, 50:, :2].copy(), dtype=torch.float32)  # (60, 2)\n",
    "        \n",
    "        # Data augmentation(only for training)\n",
    "        if self.augment:\n",
    "            if np.random.rand() < 0.5:\n",
    "                theta = np.random.uniform(-np.pi, np.pi)\n",
    "                R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                              [np.sin(theta),  np.cos(theta)]], dtype=np.float32)\n",
    "                # Rotate the historical trajectory and future trajectory\n",
    "                hist[..., :2] = hist[..., :2] @ R\n",
    "                hist[..., 2:4] = hist[..., 2:4] @ R\n",
    "                # future = future @ R gives DeprecationWarning: future a torch.Tensor\n",
    "                future = torch.from_numpy(np.dot(future.numpy(), R)) \n",
    "            if np.random.rand() < 0.5:\n",
    "                hist[..., 0] *= -1\n",
    "                hist[..., 2] *= -1\n",
    "                future[:, 0] *= -1\n",
    "\n",
    "        # Use the last timeframe of the historical trajectory as the origin\n",
    "        origin = hist[0, 49, :2].copy()  # (2,)\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        # future = future - origin -> same DeprecationWarning\n",
    "        future = torch.from_numpy(future.numpy() - origin)\n",
    "\n",
    "        # Normalize the historical trajectory and future trajectory\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "        future = future / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            y=future.type(torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "        return data_item\n",
    "    \n",
    "\n",
    "class TrajectoryDatasetTest(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Testing data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Testing data only contains historical trajectory\n",
    "        scene = self.data[idx]  # (50, 50, 6)\n",
    "        hist = scene.copy()\n",
    "        \n",
    "        origin = hist[0, 49, :2].copy()\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "        return data_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "Change which model is used at the `model = ...(input_features, output_features)` line.\n",
    "\n",
    "Change which optimizer is used at the `optimizer = optim...` line.\n",
    "\n",
    "Do **NOT** change the `criterion`, as MSE is stated in the Data tab of the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from milestone notebook\n",
    "# Set device for training speedup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Silicon GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of input features after flattening and number of output features\n",
    "# Note: LSTM models take features in different dimensions\n",
    "input_features = 50 * 50 * 6   # 50 agents, 50 time steps, 6 dimensions each (15000 input features)\n",
    "output_features = 60 * 2       # 60 future time steps, 2 dimensions (x, y) (120 output features)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "starting_lr = 1e-2\n",
    "gamma = 0.96\n",
    "scale = 10.0\n",
    "\n",
    "# Ensure this is pointing to the right model\n",
    "# OurModel = BaseLSTM(input_dim=6, hidden_dim=128, output_dim=output_features).to(device)\n",
    "OurModel = SceneContextModel(hidden_dim=864).to(device)\n",
    "\n",
    "def train_model(full_training_data: np.ndarray, \n",
    "                batch_size: int =64, epochs: int =10, num_folds: int =5,\n",
    "                early_stopping_patience: int=5, early_stopping_threshold: float=1e-3):\n",
    "\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # Perform K-fold cross validation, then pick the best model\n",
    "    best_model = None\n",
    "    overall_best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Resources used:\n",
    "    # Project milestone notebook\n",
    "    # https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md\n",
    "    for fold_i, (train_idx, val_idx) in enumerate(kfold.split(full_training_data)):\n",
    "        print(f\"\\nFOLD {fold_i + 1}/{num_folds} ==================================\")\n",
    "\n",
    "        # Prepare data from this fold\n",
    "        train_fold = full_training_data[train_idx]\n",
    "        val_fold = full_training_data[val_idx]\n",
    "        collate_func = None     # Optional for DataLoader, taken from milestone notebook\n",
    "        if isinstance(OurModel, SceneContextModel):\n",
    "            train_x = train_fold[..., :50, :]\n",
    "            train_y = train_fold[:, 0, 50:, :2]\n",
    "            X_train_tensor = torch.FloatTensor(train_x).reshape((-1, input_features))\n",
    "            y_train_tensor = torch.FloatTensor(train_y).reshape((-1, output_features))\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "            val_x = val_fold[..., :50, :]\n",
    "            val_y = val_fold[:, 0, 50:, :2]\n",
    "            X_val_tensor = torch.FloatTensor(val_x).reshape((-1, input_features))\n",
    "            y_val_tensor = torch.FloatTensor(val_y).reshape((-1, output_features))\n",
    "            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "            # Smaller LR, more patience\n",
    "            starting_lr = 1e-2\n",
    "            early_stopping_patience = 10\n",
    "        else:\n",
    "            # TrajectoryDataset expects numpy arrays\n",
    "            collate_func = lambda x: Batch.from_data_list(x)\n",
    "            train_dataset = TrajectoryDatasetTrain(train_fold, scale=scale, augment=True)\n",
    "            val_dataset = TrajectoryDatasetTrain(val_fold, scale=scale, augment=False)\n",
    "       \n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_func)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_func)\n",
    "\n",
    "        # Create the model, loss criterion, and optimizer (reset per fold, to find the best model)\n",
    "        # DO NOT CHANGE CRITERION\n",
    "        criterion = nn.MSELoss()\n",
    "        model = OurModel\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=starting_lr, weight_decay=1e-2)\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "        # How many epochs to allow for stagnant val losses (within a threshold)\n",
    "        best_val_loss = float(\"inf\")\n",
    "        no_improvement = 0\n",
    "\n",
    "        # Training loop: taken from milestone notebook\n",
    "        for epoch in tqdm(range(epochs), desc=\"Epoch\", unit=\"epoch\"):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch in train_dataloader:\n",
    "                batch_x = None\n",
    "                batch_y = None\n",
    "                if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "                    batch_x, batch_y = batch\n",
    "                else: # DataBatch type\n",
    "                    batch = batch.to(device)\n",
    "                    batch_x = batch.x\n",
    "                    batch_y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validation (fully taken from milestone notebook)\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_mae = 0\n",
    "            val_mse = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    batch_x = None\n",
    "                    batch_y = None\n",
    "                    if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "                        batch_x, batch_y = batch\n",
    "                    else: # DataBatch type\n",
    "                        print(type(batch))\n",
    "                        batch = batch.to(device)\n",
    "                        batch_x = batch.x\n",
    "                        batch_y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "\n",
    "                    pred = model(batch_x)\n",
    "                    val_loss += criterion(pred, batch_y).item()\n",
    "\n",
    "                    # show MAE and MSE with unnormalized data\n",
    "                    y = None\n",
    "                    if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "                        y = batch_y\n",
    "                    else: # DataBatch type\n",
    "                        batch_y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "                        pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "                        y = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "                    val_mae += nn.L1Loss()(pred, y).item()\n",
    "                    val_mse += nn.MSELoss()(pred, y).item()\n",
    "\n",
    "            train_loss /= len(train_dataloader)\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_mae /= len(val_dataloader)\n",
    "            val_mse /= len(val_dataloader)\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            tqdm.write(f\"Epoch {(epoch + 1):03d} | Learning rate {optimizer.param_groups[0]['lr']:.6f} | train normalized MSE {train_loss:8.4f} | val normalized MSE {val_loss:8.4f}, | val MAE {val_mae:8.4f} | val MSE {val_mse:8.4f}\")\n",
    "            if val_loss < best_val_loss - early_stopping_threshold:\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement = 0\n",
    "\n",
    "                # Better than the overall seen so far?\n",
    "                if best_val_loss < overall_best_val_loss:\n",
    "                    best_model = model\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                if no_improvement >= early_stopping_patience:\n",
    "                    print(\"Early stop!\")\n",
    "                    break\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# Train the model (tweak batch_size and epochs as needed)\n",
    "trained_model = train_model(train_data, batch_size=batch_size, epochs=50)\n",
    "\n",
    "# Define a function for prediction on the test set\n",
    "# Mostly taken from milestone notebook\n",
    "def predict(model, X_test):\n",
    "    model.eval()\n",
    "\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        if isinstance(OurModel, SceneContextModel):\n",
    "            X_test_tensor = torch.FloatTensor(X_test).reshape((-1, input_features)).to(device)\n",
    "            pred = model(X_test_tensor).cpu().reshape((-1, 60, 2))\n",
    "            pred_list.append(pred.numpy())\n",
    "        else: # Using DataBatch type from a DataLoader\n",
    "            collate_func = lambda x: Batch.from_data_list(x)\n",
    "            test_dataset = TrajectoryDatasetTest(X_test, scale=scale)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_func)\n",
    "\n",
    "            for batch in test_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred_norm = model(batch.x)\n",
    "            \n",
    "            # Reshape the prediction to (N, 60, 2)\n",
    "                pred = pred_norm * batch.scale.view(-1,1,1) + batch.origin.unsqueeze(1)\n",
    "                pred_list.append(pred.cpu().numpy())\n",
    "\n",
    "    # Reshape predictions to match submission format: (2100, 60, 2) -> (12600, 2)\n",
    "    pred_list = np.concatenate(pred_list, axis=0)  # (N,60,2)\n",
    "    pred_output = pred_list.reshape(-1, 2)  # (N*60, 2)\n",
    "    output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
    "    output_df.index.name = 'index'\n",
    "    return output_df\n",
    "\n",
    "# Make predictions on the test set\n",
    "model_predictions_df = predict(trained_model, test_data)\n",
    "\n",
    "# Save output in the submission foldder, timestamped!\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%I-%M%p\")\n",
    "\n",
    "submission_path = os.path.join(submission_dir, f\"submission-{timestamp}.csv\")\n",
    "model_predictions_df.to_csv(submission_path)\n",
    "print(f\"Submission saved locally as: '{submission_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to save and load the model (should correspond to what was trained!)\n",
    "def save_model(model, path=\"our_model.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(path=\"our_model.pth\"):\n",
    "    loaded_model = OurModel(input_features, output_features)\n",
    "    loaded_model.load_state_dict(torch.load(path))\n",
    "    loaded_model.eval()\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# save_model(model)\n",
    "# model = load_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
