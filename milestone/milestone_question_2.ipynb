{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d633ff66",
   "metadata": {},
   "source": [
    "# CSE 251B Project Milestone Problem 2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb0a627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU\n"
     ]
    }
   ],
   "source": [
    "# from starter file\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid \n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import itertools\n",
    "\n",
    "\n",
    "train_npz = np.load('./train.npz')\n",
    "train_data = train_npz['data']\n",
    "test_npz  = np.load('./test_input.npz')\n",
    "test_data  = test_npz['data']\n",
    "\n",
    "X_train = train_data[..., :50, :]\n",
    "Y_train = train_data[:, 0, 50:, :2]\n",
    "\n",
    "class TrajectoryDatasetTrain(Dataset):\n",
    "    def __init__(self, data, scale=10.0, augment=True):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Training data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        augment: Whether to apply data augmentation (only for training)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]\n",
    "        # Getting 50 historical timestamps and 60 future timestamps\n",
    "        hist = scene[:, :50, :].copy()    # (agents=50, time_seq=50, 6)\n",
    "        future = torch.tensor(scene[0, 50:, :2].copy(), dtype=torch.float32)  # (60, 2)\n",
    "        \n",
    "        # Data augmentation(only for training)\n",
    "        if self.augment:\n",
    "            if np.random.rand() < 0.5:\n",
    "                theta = np.random.uniform(-np.pi, np.pi)\n",
    "                R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                              [np.sin(theta),  np.cos(theta)]], dtype=np.float32)\n",
    "                # Rotate the historical trajectory and future trajectory\n",
    "                hist[..., :2] = hist[..., :2] @ R\n",
    "                hist[..., 2:4] = hist[..., 2:4] @ R\n",
    "                future = future @ R\n",
    "            if np.random.rand() < 0.5:\n",
    "                hist[..., 0] *= -1\n",
    "                hist[..., 2] *= -1\n",
    "                future[:, 0] *= -1\n",
    "\n",
    "        # Use the last timeframe of the historical trajectory as the origin\n",
    "        origin = hist[0, 49, :2].copy()  # (2,)\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        future = future - origin\n",
    "\n",
    "        # Normalize the historical trajectory and future trajectory\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "        future = future / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            y=future.type(torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "        return data_item\n",
    "    \n",
    "\n",
    "class TrajectoryDatasetTest(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Testing data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Testing data only contains historical trajectory\n",
    "        scene = self.data[idx]  # (50, 50, 6)\n",
    "        hist = scene.copy()\n",
    "        \n",
    "        origin = hist[0, 49, :2].copy()\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "        return data_item\n",
    "\n",
    "torch.manual_seed(251)\n",
    "np.random.seed(42)\n",
    "\n",
    "scale = 7.0\n",
    "\n",
    "N = len(train_data)\n",
    "val_size = int(0.1 * N)\n",
    "train_size = N - val_size\n",
    "\n",
    "train_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
    "val_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "\n",
    "# Set device for training speedup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Silicon GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9fe51",
   "metadata": {},
   "source": [
    "### Problem 2A - Searching for the optimal parameters to train our model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(\n",
    "    train_data: np.ndarray,\n",
    "    model_class,\n",
    "    model_kwargs: Dict = {},\n",
    "    search_params: Dict = None,\n",
    "    batch_size: int = 32,\n",
    "    val_ratio: float = 0.1,\n",
    "    scale: float = 7.0,\n",
    "    max_epochs: int = 30,\n",
    "    early_stopping_patience: int = 5,\n",
    "    device: torch.device = None,\n",
    "    results_dir: str = 'hyperparameter_search_results',\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter search for trajectory prediction model.\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training data\n",
    "        model_class: Model class to instantiate\n",
    "        model_kwargs: Additional model parameters\n",
    "        search_params: Dictionary of parameters to search over\n",
    "        batch_size: Batch size for training\n",
    "        val_ratio: Proportion of data to use for validation\n",
    "        scale: Scale factor for normalization\n",
    "        max_epochs: Maximum number of epochs to train\n",
    "        early_stopping_patience: Number of epochs to wait before early stopping\n",
    "        device: Device to train on\n",
    "        results_dir: Directory to save results\n",
    "        verbose: Whether to print progress\n",
    "        \n",
    "    Returns:\n",
    "        results_df: DataFrame of results\n",
    "        best_params: Best parameters found\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "            print(\"Using Apple Silicon GPU\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            print(\"Using CUDA GPU\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print(\"Using CPU\")\n",
    "    \n",
    "    if search_params is None:\n",
    "        search_params = {\n",
    "            'learning_rate': [1e-2, 1e-3, 5e-4],\n",
    "            'weight_decay': [1e-4, 1e-5, 0.0],\n",
    "            'step_size': [10, 20, 30],\n",
    "            'gamma': [0.5, 0.25, 0.1]\n",
    "        }\n",
    "    \n",
    "    # Create directory for results\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare the data\n",
    "    N = len(train_data)\n",
    "    val_size = int(val_ratio * N)\n",
    "    train_size = N - val_size\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
    "    val_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=lambda x: Batch.from_data_list(x)\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=lambda x: Batch.from_data_list(x)\n",
    "    )\n",
    "    \n",
    "    # Generate parameter combinations\n",
    "    param_grid = list(ParameterGrid(search_params))\n",
    "    if verbose:\n",
    "        print(f\"Testing {len(param_grid)} parameter combinations\")\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = []\n",
    "    \n",
    "    # Run hyperparameter search\n",
    "    for param_idx, params in enumerate(param_grid):\n",
    "        if verbose:\n",
    "            print(f\"\\nParameter set {param_idx+1}/{len(param_grid)}: {params}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = model_class(**model_kwargs).to(device)\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=params['step_size'],\n",
    "            gamma=params['gamma']\n",
    "        )\n",
    "        \n",
    "        # Track best validation loss\n",
    "        best_val_loss = float('inf')\n",
    "        no_improvement = 0\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_maes = []\n",
    "        val_mses = []\n",
    "        epoch_times = []\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(max_epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # ---- Training ----\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch in train_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch)\n",
    "                y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "                loss = criterion(pred, y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # ---- Validation ----\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_mae = 0\n",
    "            val_mse = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    batch = batch.to(device)\n",
    "                    pred = model(batch)\n",
    "                    y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "                    val_loss += criterion(pred, y).item()\n",
    "                    \n",
    "                    # Unnormalize for metric calculation\n",
    "                    pred_unnorm = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "                    y_unnorm = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "                    \n",
    "                    val_mae += nn.L1Loss()(pred_unnorm, y_unnorm).item()\n",
    "                    val_mse += nn.MSELoss()(pred_unnorm, y_unnorm).item()\n",
    "            \n",
    "            # Calculate average losses\n",
    "            train_loss /= len(train_dataloader)\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_mae /= len(val_dataloader)\n",
    "            val_mse /= len(val_dataloader)\n",
    "            \n",
    "            # Step scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track time\n",
    "            epoch_time = time.time() - start_time\n",
    "            epoch_times.append(epoch_time)\n",
    "            \n",
    "            # Track metrics\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_maes.append(val_mae)\n",
    "            val_mses.append(val_mse)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch:03d} | LR {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "                      f\"Train MSE {train_loss:.4f} | Val MSE {val_loss:.4f} | \"\n",
    "                      f\"Val MAE {val_mae:.4f} | Val ADE {val_mae:.4f} | \"\n",
    "                      f\"Time {epoch_time:.2f}s\")\n",
    "            \n",
    "            # Check for improvement\n",
    "            if val_loss < best_val_loss - 1e-3:\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                if no_improvement >= early_stopping_patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        # Record results for this parameter set\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        best_epoch = val_losses.index(min(val_losses))\n",
    "        \n",
    "        param_results = {\n",
    "            **params,\n",
    "            'best_val_loss': min(val_losses),\n",
    "            'best_val_mae': val_maes[best_epoch],\n",
    "            'best_val_mse': val_mses[best_epoch],\n",
    "            'best_epoch': best_epoch,\n",
    "            'epoch_time_seconds': avg_epoch_time,\n",
    "            'total_epochs': len(train_losses)\n",
    "        }\n",
    "        \n",
    "        results.append(param_results)\n",
    "        \n",
    "        # Create and save training curve plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.title('Training Curves')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(val_maes, label='MAE (meters)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        lr_values = [optimizer.param_groups[0]['lr'] * (params['gamma'] ** (i // params['step_size'])) \n",
    "                     for i in range(len(train_losses))]\n",
    "        plt.plot(lr_values)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.bar(['Learning Rate', 'Weight Decay', 'Step Size', 'Gamma'], \n",
    "                [params['learning_rate'], params['weight_decay'], params['step_size'], params['gamma']])\n",
    "        plt.title('Hyperparameters')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{results_dir}/params_{param_idx+1}_plot.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by validation MAE\n",
    "    results_df = results_df.sort_values('best_val_mae')\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(f\"{results_dir}/hyperparameter_search_results.csv\", index=False)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = results_df.iloc[0].to_dict()\n",
    "    \n",
    "    # Plot comparative results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    param_labels = [f\"lr={p['learning_rate']:.0e}, wd={p['weight_decay']:.0e}, ss={p['step_size']}, γ={p['gamma']}\" \n",
    "                   for _, p in results_df.iloc[:10].iterrows()]\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.barh(param_labels, results_df['best_val_mae'].iloc[:10])\n",
    "    plt.xlabel('MAE (meters)')\n",
    "    plt.title('Top 10 Parameter Sets by MAE')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.barh(param_labels, results_df['best_val_mse'].iloc[:10])\n",
    "    plt.xlabel('MSE (square meters)')\n",
    "    plt.title('MSE for Top 10 Parameter Sets')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.barh(param_labels, results_df['epoch_time_seconds'].iloc[:10])\n",
    "    plt.xlabel('Average Epoch Time (seconds)')\n",
    "    plt.title('Training Time for Top 10 Parameter Sets')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.barh(param_labels, results_df['best_epoch'].iloc[:10])\n",
    "    plt.xlabel('Best Epoch')\n",
    "    plt.title('Convergence Speed for Top 10 Parameter Sets')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{results_dir}/comparative_results.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nHyperparameter search complete!\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    return results_df, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1c09e",
   "metadata": {},
   "source": [
    "### Problem 2B - Deciding which model we should use to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe58379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim=50 * 50 * 2, output_dim=60 * 2):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x[..., :2] # (batch*50, 50, 2)\n",
    "        x = x.reshape(-1, 50 * 50 * 2) # (batch, 5000)\n",
    "        x = self.linear(x)\n",
    "        return x.view(-1, 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(256, output_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        if len(x.shape) == 4:\n",
    "            x = x[:, :, :, :2] # (batch, 50, 50, 2)\n",
    "            # x = x.reshape(-1, 50 * 50 * 6)\n",
    "            x = x.reshape(x.size(0), -1) # (batch, 50 * 50 * 6)\n",
    "        else:\n",
    "            x = self.flatten(x)\n",
    "        x = self.mlp(x)\n",
    "        return x.view(-1, 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=128, output_dim=60 * 2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x= x.reshape(-1, 50, 50, 6)  # (batch_size, num_agents, seq_len, input_dim)\n",
    "        x = x[:, 0, :, :] # Only Consider ego agent index 0\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out.view(-1, 60, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milestone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
